{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"display: flex; align-items: left;\">\n",
        "    <a href=\"https://sites.google.com/corp/google.com/genai-solutions/home?authuser=0\">\n",
        "        <img src=\"https://storage.googleapis.com/miscfilespublic/Linkedin%20Banner%20%E2%80%93%202.png\" style=\"margin-right\">\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRyGcAepAPJ5"
      },
      "source": [
        "\n",
        "<h1 align=\"center\">Open Data QnA - Chat with your SQL Database</h1> \n",
        "\n",
        "---\n",
        "\n",
        "This notebook first walks through the Vector Store Setup needed for running the Open Data QnA application. \n",
        "\n",
        "Currently supported Source DBs are: \n",
        "- PostgreSQL on Google Cloud SQL \n",
        "- BigQuery\n",
        "\n",
        "Furthermore, the following vector stores are supported \n",
        "- pgvector on PostgreSQL \n",
        "- BigQuery vector\n",
        "\n",
        "\n",
        "The setup part covers the following steps: \n",
        "> 1. Configuration: Intial GCP project, IAM permissions, Environment  and Databases setup including logging on Bigquery for analytics\n",
        "\n",
        "> 2. Creation of Table, Column and Known Good Query Embeddings in the Vector Store  for Retreival Augmented Generation(RAG)\n",
        "\n",
        "> 3. Setting up firestore for persisting the session history for multiturn\n",
        "\n",
        "\n",
        "Afterwards, you will be able to run the Open Data QnA Pipeline to generate SQL queries and answer questions over your data source. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsWGZW_fUJjN"
      },
      "source": [
        "### üìí Using this interactive notebook\n",
        "\n",
        "If you have not used this IDE with jupyter notebooks it will ask for installing Python + Jupyter extensions. Please go ahead install them\n",
        "\n",
        "Click the **run** icons ‚ñ∂Ô∏è  of each cell within this notebook.\n",
        "\n",
        "> üí° Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `‚åò + Enter` on a Mac).\n",
        "\n",
        "> ‚ö†Ô∏è **To avoid any errors**, wait for each section to finish in their order before clicking the next ‚Äúrun‚Äù icon.\n",
        "\n",
        "This sample must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
        "\n",
        "You can use an existing project. Alternatively, you can create a new Cloud project [with cloud credits for free.](https://cloud.google.com/free/docs/gcp-free-tier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RicDCkdI-hmp"
      },
      "source": [
        "# üöß **0. Prerequisites**\n",
        "\n",
        "Make sure that Google Cloud CLI is installed before moving to the next cell! You can refer to the link below for guidance\n",
        "\n",
        "Installation Guide: https://cloud.google.com/sdk/docs/install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  **0.1. Setup Poetry Environment and Setup GCP Project** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üíª **Install Code Dependencies (Create and setup venv)**\n",
        "Install the dependencies by runnign the poetry commands below \n",
        "\n",
        "Note: Below command runs with default Python Kernel and we will change that to Kernel from venv after this execution below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install poetry\n",
        "! pip uninstall poetry -y\n",
        "! pip install poetry --quiet\n",
        "\n",
        "#Run the poetry commands below to set up the environment\n",
        "!poetry lock #resolve dependecies (also auto create poetry venv if not exists)\n",
        "!poetry install --quiet #installs dependencies\n",
        "!poetry env info #Displays the evn just created and the path to it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìå **Important Step: Activate your virtual environment and authenticate with Google Cloud CLI**\n",
        "\n",
        "#### **All commands in this cell to be run on Terminal**\n",
        "\n",
        "\n",
        "Chose the relevant instructions based on where you are running this notebook\n",
        "\n",
        "\n",
        "**For IDEs like Cloud Shell Editor, VS Code**\n",
        "\n",
        "Once the venv created either in the local directory or in the cache directory. Open the terminal on the same machine where your notebooks are running and start running the below commands.\n",
        "\n",
        "\n",
        "```\n",
        "poetry shell #this command should activate your venv and you should see it enters into the venv\n",
        "\n",
        "##inside the activated venv shell []\n",
        "\n",
        "gcloud auth login\n",
        "\n",
        "gcloud services enable \\\n",
        "    serviceusage.googleapis.com \\\n",
        "    cloudresourcemanager.googleapis.com --project <<Enter Project Id>>\n",
        "\n",
        "```\n",
        "For IDEs adding Juypter Extensions will automatically give you option to change the kernel. If not, manually select the python interpreter in your IDE (The exact is shown in the above cell. Path would look like e.g. /home/admin_/Talk2Data/.venv/bin/python or ~cache/user/Talk2Data/.venv/bin/python)\n",
        "\n",
        "\n",
        "**For Jupyter Lab or Jupyter Environments on Workbench etc**\n",
        "\n",
        "\n",
        "```\n",
        "poetry shell #this command should activate your venv and you should see it enters into the venv\n",
        "\n",
        "##inside the activated venv shell []\n",
        "\n",
        "#If you are running on Worbench instance where the service account used has required permissions to run this solution you can skip the below gcloud auth commands and get to next kernel creation section\n",
        "\n",
        "gcloud auth application-default login\n",
        "gcloud services enable \\\n",
        "    serviceusage.googleapis.com \\\n",
        "    cloudresourcemanager.googleapis.com --project <<Enter Project Id>>\n",
        "gcloud auth application-default set-quota-project <<Enter Project Id for using resources>>\n",
        "\n",
        "```\n",
        "\n",
        "Create Kernel for with the envrionment created\n",
        "\n",
        "```\n",
        "\n",
        "pip install jupyter\n",
        "\n",
        "ipython kernel install --name \"openqna-venv\" --user \n",
        "\n",
        "```\n",
        "\n",
        "Restart your kernel or close the exsiting notebook and open again, you should now see the \"openqna-venv\" in the kernel drop down\n",
        "\n",
        "**What did we do here?**\n",
        "\n",
        "* Created Application Default Credentials to use for the code\n",
        "* Added venv to kernel to select for runningt the notebooks (For standalone Jupyter setups like Workbench etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Change your Kernel to the above created .venv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Python Module Path to Root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4W6FPnrYEE8"
      },
      "source": [
        "### üîó **Connect Your Google Cloud Project**\n",
        "Time to connect your Google Cloud Project to this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Project has been set to three-p-o\n"
          ]
        }
      ],
      "source": [
        "#@markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
        "PROJECT_ID = \"three-p-o\"\n",
        "\n",
        "# Quick input validations.\n",
        "assert PROJECT_ID, \"‚ö†Ô∏è Please provide your Google Cloud Project ID\"\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "print(f'Project has been set to {PROJECT_ID}')\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_QUOTA_PROJECT']=PROJECT_ID\n",
        "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID\n",
        "\n",
        "#If errors out for authentication restart the kernel and start from the previous cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è **Enable Required API Services in the GCP Project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Operation \"operations/acat.p2-978842762722-93441efe-e4b8-43c9-9578-b4652dc1d301\" finished successfully.\n"
          ]
        }
      ],
      "source": [
        "#Enable all the required APIs for the Open Data QnA solution\n",
        "\n",
        "!gcloud services enable \\\n",
        "  cloudapis.googleapis.com \\\n",
        "  compute.googleapis.com \\\n",
        "  iam.googleapis.com \\\n",
        "  run.googleapis.com \\\n",
        "  sqladmin.googleapis.com \\\n",
        "  aiplatform.googleapis.com \\\n",
        "  bigquery.googleapis.com \\\n",
        "  firestore.googleapis.com --project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Configure your inputs for the environments**\n",
        "\n",
        "This section assumes that a datasource is already set up in your GCP project. If a datasource has not been set up, use the notebooks below to copy a public data set from BigQuery to Cloud SQL or BigQuery on your GCP project\n",
        "\n",
        "\n",
        "Enabled Data Sources:\n",
        "* PostgreSQL on Google Cloud SQL (Copy Sample Data: [0_CopyDataToCloudSqlPG.ipynb](0_CopyDataToCloudSqlPG.ipynb))\n",
        "* BigQuery (Copy Sample Data: [0_CopyDataToBigQuery.ipynb](0_CopyDataToBigQuery.ipynb))\n",
        "\n",
        "Enabled Vector Stores:\n",
        "* pgvector on PostgreSQL \n",
        "* BigQuery vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î **Choose Data Source and Vector Store**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Sources to connect**\n",
        "- This solution lets you setup multiple data source at the same time.\n",
        "- You can group multiple tables from different datasets or schema into a grouping and provide the details\n",
        "- If your dataset/schema has many tables and you want to run the solution against few you should specifically choose a group for that tables only\n",
        "\n",
        "**Format for data_source_list.csv**\n",
        "\n",
        "**source | user_grouping | schema | table**\n",
        "\n",
        "**source** - Supported Data Sources. #Options: bigquery , cloudsql-pg\n",
        "\n",
        "**user_grouping** - Logical grouping or use case name for tables from same or different schema/dataset. When left black it default to the schema value in the next column\n",
        "\n",
        "**schema** - schema name for postgres or dataset name in bigquery \n",
        "\n",
        "**table** - name of the tables to run the solutions against. Leave this column blank after filling schema/dataset if you want to run solution for whole dataset/schema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fill out the parameters and configuration settings below. \n",
        "These are the parameters for setting configurations for the vector store tables to be created. \n",
        "\n",
        "Additionally, you can specify whether you have and want to use known-good-queries for the pipeline run and whether you want to enable logging.\n",
        "\n",
        "**Known good queries:** if you have known working user question <-> SQL query pairs, you can put them into the file `scripts/known_good_sql.csv`. This will be used as a caching layer and for in-context learning: If an exact match of the user question is found in the vector store, the pipeline will skip SQL Generation and output the cached SQL query. If the similarity score is between 90-100%, the known good queries will be used as few-shot examples by the SQL Generator Agent. \n",
        "\n",
        "**Logging:** you can enable logging. If enabled, a dataset is created in Big Query in your project, which will store the logging table and save information from the pipeline run in the logging table. This is especially helpful for debugging.\n",
        "\n",
        "**use_column_samples:** you can enable use column samples flag to let the pipeline select sample values of the columns from the source database. In some specific usecase where we need to understand the format or case sensitivity of the values this flag help LLM to have better understanding. Though this is one time setup, please be aware that turning this on mean getting samples from each column and it can be an expensive operation when there are lot many columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#[CONFIG]\n",
        "embedding_model = 'vertex' # Options: 'vertex' or 'vertex-lang'\n",
        "description_model = 'gemini-1.5-pro' # 'gemini-1.0-pro', 'gemini-1.5-pro', 'text-bison-32k', 'gemini-1.5-flash'\n",
        "vector_store = 'bigquery-vector' # Options: 'bigquery-vector', 'cloudsql-pgvector'\n",
        "logging = True # True or False \n",
        "kgq_examples = True # True or False\n",
        "use_column_samples = True #True or False\n",
        "\n",
        "#[GCP]\n",
        "project_id = PROJECT_ID\n",
        "\n",
        "#[PGCLOUDSQL]\n",
        "# Default values for pgvector setup, change only if needed\n",
        "pg_region = 'us-central1'\n",
        "pg_instance = 'pg15-opendataqna'\n",
        "pg_database = 'opendataqna-db'\n",
        "pg_user = 'pguser'\n",
        "pg_password = 'pg123'\n",
        "\n",
        "#[BIGQUERY]\n",
        "# Name for the BQ dataset created for bigquery-vector and/or logging. Change names only if needed.\n",
        "bq_dataset_region = 'us-central1'\n",
        "bq_opendataqna_dataset_name = 'opendataqna'\n",
        "bq_log_table_name = 'audit_log_table'\n",
        "\n",
        "#Details for firestore to store the chat session history\n",
        "firestore_region='us-central1'\n",
        "## firestore_database is named as 'opendataqna-session-logs' (This is designed to not be customizable as the setup includes creation of composite indexes from backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick input verifications below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Input verification - Vector Store\n",
        "assert vector_store in {'bigquery-vector', 'cloudsql-pgvector'}, \"‚ö†Ô∏è Invalid VECTOR_STORE. Must be 'bigquery-vector' or 'cloudsql-pgvector'\"\n",
        "\n",
        "# Input verification - Firestore Region\n",
        "assert firestore_region, \"‚ö†Ô∏è Provide firestore region name\"\n",
        "\n",
        "if logging: \n",
        "    assert bq_log_table_name, \"‚ö†Ô∏è Please provide a name for your log table if you want to use logging\"\n",
        "\n",
        "if vector_store == 'bigquery':\n",
        "    assert bq_dataset_region, \"‚ö†Ô∏è Please provide the Data Set Region\"\n",
        "    assert bq_opendataqna_dataset_name, \"‚ö†Ô∏è Please provide the name of the logging/vector store dataset on Bigquery\"\n",
        "\n",
        "elif vector_store == 'cloudsql-pg':\n",
        "    assert pg_region, \"‚ö†Ô∏è Please provide Region of the Cloud SQL Instance\"\n",
        "    assert pg_instance, \"‚ö†Ô∏è Please provide the name of the Cloud SQL Instance\"\n",
        "    assert pg_database, \"‚ö†Ô∏è Please provide the name of the PostgreSQL Database on the Cloud SQL Instance\"\n",
        "    assert pg_user, \"‚ö†Ô∏è Please provide a username for the Cloud SQL Instance\"\n",
        "    assert pg_password, \"‚ö†Ô∏è Please provide the Password for the PG_USER\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üíæ **Save Configuration to File** \n",
        "Save the configurations set in this notebook to  `config.ini`. The parameters from this file are used in notebooks and in various modeules in the repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current dir:  /home/admin_/Open_Data_QnA/notebooks\n",
            "All configuration paramaters saved to file!\n"
          ]
        }
      ],
      "source": [
        "from scripts import save_config\n",
        "\n",
        "save_config(embedding_model, description_model, vector_store, logging, kgq_examples, use_column_samples, PROJECT_ID,\n",
        "            pg_region, pg_instance, pg_database, pg_user, pg_password, \n",
        "            bq_dataset_region, bq_opendataqna_dataset_name, bq_log_table_name,firestore_region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **1. Vector Store Setup** (Run once)\n",
        "---\n",
        "\n",
        "This section walks through the Vector Store Setup needed for running the Open Data QnA application. \n",
        "\n",
        "It covers the following steps: \n",
        "> 1. Configuration: Environment and Databases setup including logging on Bigquery for analytics\n",
        "\n",
        "> 2. Creation of Table, Column and Known Good Query Embeddings in the Vector Store  for Retreival Augmented Generation(RAG)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è **1.1 Database Setup for Vector Store: CloudSQL-pgvector**\n",
        "\n",
        "Create PostgreSQL Instance on CloudSQL if 'cloudsql-pgvector' is chosen as vector store\n",
        "\n",
        "Note that a PostgreSQL Instance on CloudSQL already exists if 'cloudsql-pg' is the data source. PostgreSQL Instance is created only if a different data store is chosen.\n",
        "\n",
        "The cell will also create a dataset to store the log table on Big Query, **if** logging is enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing environment setup.\n",
            "Loading configurations from config.ini file.\n",
            "Vector Store source set to:  bigquery-vector\n",
            "Logging is enabled\n",
            "Vector store set to 'bigquery-vector'\n",
            "Generating Big Query dataset opendataqna\n",
            "Destination Dataset exists\n"
          ]
        }
      ],
      "source": [
        "from env_setup import create_vector_store\n",
        "# Setup vector store for embeddings\n",
        "create_vector_store()  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  **1.2 Create Embeddings in Vector Store for RAG** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üñãÔ∏è **Create Table and Column Embeddings**\n",
        "\n",
        "In this step, table and column metadata is retreived from the data source and embeddings are generated for both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current dir:  /home/admin_/Open_Data_QnA/notebooks\n",
            "root_dir set to: /home/admin_/Open_Data_QnA\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1721717874.012655    9216 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings from source db schemas\n",
            "Source Found at Path :: /home/admin_/Open_Data_QnA/scripts/data_source_list.csv\n",
            "The Embeddings are extracted for the below combinations\n",
            "     source        schema                                              table\n",
            "0  bigquery   imdb_people  [name_basics, title_basics, title_crew, title_...\n",
            "1  bigquery  imdb_ratings                           [reviews, title_ratings]\n",
            "\n",
            "\n",
            "\n",
            "Generated table description for imdb_people.name_basics\n",
            "Generated table description for imdb_people.title_crew\n",
            "Generated table description for imdb_people.title_basics\n",
            "Generated table description for imdb_people.title_principals\n",
            "LLM generated 4 Table Descriptions\n",
            "\n",
            "\n",
            "\n",
            "LLM generated 0 Column Descriptions\n",
            "\n",
            "\n",
            "\n",
            "Generated table description for imdb_ratings.reviews\n",
            "Generated table description for imdb_ratings.title_ratings\n",
            "LLM generated 2 Table Descriptions\n",
            "\n",
            "\n",
            "\n",
            "LLM generated 0 Column Descriptions\n",
            "Table and Column embeddings are created\n"
          ]
        }
      ],
      "source": [
        "from env_setup import get_embeddings\n",
        "\n",
        "# Generate embeddings for tables and columns\n",
        "table_schema_embeddings, col_schema_embeddings = get_embeddings()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üíæ **Save the Table and Column Embeddings in the Vector Store**\n",
        "The table and column embeddings created in the above step are save to the Vector Store chosen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Storing embeddings back to the vector store.\n",
            "Table and Column embeddings are saved to vector store\n"
          ]
        }
      ],
      "source": [
        "from env_setup import store_embeddings\n",
        "\n",
        "# Store table/column embeddings (asynchronous)\n",
        "await(store_embeddings(table_schema_embeddings, col_schema_embeddings))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **2. Firestore Database Setup**\n",
        "---\n",
        "\n",
        "This section walks through setting up the firestore DB to store the session history of the conversation for multiturn\n",
        "\n",
        "It covers the following steps: \n",
        "> 1. Creation Firestore Database\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing Firestore database with this name already!\n"
          ]
        }
      ],
      "source": [
        "from env_setup import create_firestore_db\n",
        "\n",
        "create_firestore_db(firestore_region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÑÔ∏è **3. Load Known Good SQL into Vector Store**\n",
        "Known Good Queries are used to create query cache for Few shot examples. Creating a query cache is highly recommended for best outcomes! \n",
        "\n",
        "The following cell will load the Natural Language Question and Known Good SQL pairs into our Vector Store. There pairs are loaded from `known_good_sql.csv` file inside scripts folder. If you have your own Question-SQL examples, curate them in .csv file before running the cell below. \n",
        "\n",
        "If no Known Good Queries are available at this time to create query cache, you can use [3_LoadKnownGoodSQL.ipynb](3_LoadKnownGoodSQL.ipynb) to load them later. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Format of the Known Good SQL File (known_good_sql.csv)\n",
        "\n",
        "prompt | sql | user_grouping [3 columns]\n",
        "\n",
        "prompt ==> User Question \n",
        "\n",
        "sql ==> SQL for the user question (Note that the sql should enclosed in quotes and only in single line. Please remove the line  break)\n",
        "\n",
        "user_grouping ==>This name should exactly  match the grouping name you mentioned while creating vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating kgq table in vector store.\n",
            "Reading contents of known_good_sql.csv\n",
            "Storing kgq embeddings in vector store table.\n",
            " LIMIT 1Y midyear_population DESC.midyear_population\n"
          ]
        },
        {
          "ename": "BadRequest",
          "evalue": "400 POST https://bigquery.googleapis.com/bigquery/v2/projects/three-p-o/queries?prettyPrint=false: Syntax error: Unclosed string literal at [3:21]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m(create_kgq_sql_table()) \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Store known good query embeddings (if enabled)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m(store_kgq_sql_embeddings())  \n",
            "File \u001b[0;32m~/Open_Data_QnA/env_setup.py:409\u001b[0m, in \u001b[0;36mstore_kgq_sql_embeddings\u001b[0;34m()\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Add KGQ to the vector store\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m VECTOR_STORE\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigquery-vector\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m(store_kgq_embeddings(df_kgq,\n\u001b[1;32m    410\u001b[0m                             project_id\u001b[38;5;241m=\u001b[39mPROJECT_ID,\n\u001b[1;32m    411\u001b[0m                                 instance_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    412\u001b[0m                                 database_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    413\u001b[0m                                 schema\u001b[38;5;241m=\u001b[39mBQ_OPENDATAQNA_DATASET_NAME,\n\u001b[1;32m    414\u001b[0m                                 database_user\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m                                 database_password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    416\u001b[0m                                 region\u001b[38;5;241m=\u001b[39mBQ_REGION,\n\u001b[1;32m    417\u001b[0m                                 VECTOR_STORE \u001b[38;5;241m=\u001b[39m VECTOR_STORE\n\u001b[1;32m    418\u001b[0m                                 ))\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m VECTOR_STORE\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloudsql-pgvector\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m(store_kgq_embeddings(df_kgq,\n\u001b[1;32m    422\u001b[0m                             project_id\u001b[38;5;241m=\u001b[39mPROJECT_ID,\n\u001b[1;32m    423\u001b[0m                                 instance_name\u001b[38;5;241m=\u001b[39mPG_INSTANCE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                 VECTOR_STORE \u001b[38;5;241m=\u001b[39m VECTOR_STORE\n\u001b[1;32m    430\u001b[0m                                 ))\n",
            "File \u001b[0;32m~/Open_Data_QnA/embeddings/kgq_embeddings.py:111\u001b[0m, in \u001b[0;36mstore_kgq_embeddings\u001b[0;34m(df_kgq, project_id, instance_name, database_name, schema, database_user, database_password, region, VECTOR_STORE)\u001b[0m\n\u001b[1;32m    109\u001b[0m             cleaned_sql \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_generated_sql\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m             \u001b[38;5;28mprint\u001b[39m(cleaned_sql)\n\u001b[0;32m--> 111\u001b[0m             \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_and_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[38;5;124;43mINSERT INTO `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.example_prompt_sql_embeddings` \u001b[39;49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;43m                VALUES (\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexample_grouping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexample_user_question\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m , \u001b[39;49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;43m                \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcleaned_sql\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m )\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m VECTOR_STORE\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloudsql-pgvector\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    120\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/cloud/bigquery/client.py:3601\u001b[0m, in \u001b[0;36mClient.query_and_wait\u001b[0;34m(self, query, job_config, location, project, api_timeout, wait_timeout, retry, job_retry, page_size, max_results)\u001b[0m\n\u001b[1;32m   3595\u001b[0m     _verify_job_config_type(job_config, QueryJobConfig)\n\u001b[1;32m   3597\u001b[0m job_config \u001b[38;5;241m=\u001b[39m _job_helpers\u001b[38;5;241m.\u001b[39mjob_config_with_defaults(\n\u001b[1;32m   3598\u001b[0m     job_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_query_job_config\n\u001b[1;32m   3599\u001b[0m )\n\u001b[0;32m-> 3601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_job_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_and_wait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_retry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3613\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/cloud/bigquery/_job_helpers.py:509\u001b[0m, in \u001b[0;36mquery_and_wait\u001b[0;34m(client, query, job_config, location, project, api_timeout, wait_timeout, retry, job_retry, page_size, max_results)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39mRowIterator(\n\u001b[1;32m    493\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    494\u001b[0m         api_request\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(client\u001b[38;5;241m.\u001b[39m_call_api, retry, timeout\u001b[38;5;241m=\u001b[39mapi_timeout),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m         num_dml_affected_rows\u001b[38;5;241m=\u001b[39mquery_results\u001b[38;5;241m.\u001b[39mnum_dml_affected_rows,\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_retry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjob_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do_query()\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/cloud/bigquery/_job_helpers.py:450\u001b[0m, in \u001b[0;36mquery_and_wait.<locals>.do_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# For easier testing, handle the retries ourselves.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 450\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We're calling the retry decorator ourselves.\u001b[39;49;00m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBigQuery.query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39m_call_api(\n\u001b[1;32m    461\u001b[0m         retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    462\u001b[0m         span_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigQuery.query\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mapi_timeout,\n\u001b[1;32m    468\u001b[0m     )\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/cloud/bigquery/client.py:833\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[1;32m    831\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[1;32m    832\u001b[0m     ):\n\u001b[0;32m--> 833\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
            "File \u001b[0;32m~/Open_Data_QnA/.venv/lib/python3.10/site-packages/google/cloud/_http/__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/three-p-o/queries?prettyPrint=false: Syntax error: Unclosed string literal at [3:21]"
          ]
        }
      ],
      "source": [
        "from env_setup import create_kgq_sql_table, store_kgq_sql_embeddings\n",
        "\n",
        "# Create table for known good queries (if enabled)\n",
        "await(create_kgq_sql_table()) \n",
        "\n",
        "# Store known good query embeddings (if enabled)\n",
        "await(store_kgq_sql_embeddings())  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü•Å If all the above steps are executed suucessfully, the following should be set up:\n",
        "\n",
        "* GCP project and all the required IAM permissions\n",
        "\n",
        "* Environment to run the solution\n",
        "\n",
        "* Data source and Vector store for the solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__________________________________________________________________________________________________________________"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
